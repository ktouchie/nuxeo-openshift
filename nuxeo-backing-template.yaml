apiVersion: v1
kind: Template
message: |-
  The following backing services for Nuxeo have been created in your project:

    * a MongoDB cluster
    * an Elasticsearch Cluster
    * a Kafka Cluster backed by Zookeeper

  These services can be used by a Nuxeo cluster. You can use the `nuxeo-cluster` and 
  refer the backing services name : ${APPLICATION_NAME} 
  
  For more information about using this template, see https://github.com/nuxeo/nuxeo-openshift
metadata:  
  name: nuxeo-backings
  namespace: openshift
  annotations:
    description: |
      This templates setup the backing service needed to run a Nuxeo cluster. Once  MongoDB, ElasticSearch and Kafka are 
      started and ready, you can start one of the Nuxeo cluster flavor (image based or S2i based).
    openshift.io/display-name: Backing services for Nuxeo
    template.openshift.io/documentation-url: https://github.com/nuxeo/nuxeo-openshift
    template.openshift.io/long-description: |-
      This templates setup the backing service needed to run a Nuxeo cluster. Once  MongoDB, ElasticSearch and Kafka are 
      started and ready, you can start one of the Nuxeo cluster flavor (image based or S2i based).
    template.openshift.io/provider-display-name: Nuxeo
    template.openshift.io/support-url: https://answers.nuxeo.com/
    iconClass: icon-java
    tags: java, nuxeo
parameters:
  - description: The name for the application.
    name: APPLICATION_NAME
    value: nuxeo-backings
    required: true
  - description: Size of persistent storage for MongoDB.
    name: VOLUME_MONGODB_CAPACITY
    value: 5Gi
    required: true
  - description: Name of the MongoDB replica set
    name: MONGODB_REPLICASET_NAME
    value: rs0
    required: true
  - description: Size of persistent storage for Elasticsearch.
    name: VOLUME_ELASTICSEARCH_CAPACITY
    value: 10Gi
    required: true
  - description: Elasticsearch cluster name
    name: ELASTICSEARCH_CLUSTER_NAME
    value: nuxeo
    required: true
  - description: Elasticsearch Memory
    name: ELASTICSEARCH_CLUSTER_MEMORY
    value: 256m
    required: true
  - description: Size of persistent storage for Zookeper.
    name: VOLUME_ZOOKEEPER_CAPACITY
    value: 10Gi
  - description: Size of persistent storage for Kafka.
    name: VOLUME_KAFKA_CAPACITY
    value: 10Gi


objects:

- apiVersion: v1
  kind: ConfigMap
  metadata:
    name: ${APPLICATION_NAME}-elasticsearch-config
    labels:
      app: ${APPLICATION_NAME}
      component: elasticsearch
      role: config
  data:
    elasticsearch.yml: |-
      cluster.name: ${ELASTICSEARCH_CLUSTER_NAME}      
      
      node.data: ${NODE_DATA:true}
      node.master: ${NODE_MASTER:true}
      node.ingest: ${NODE_INGEST:true}
      node.name: ${HOSTNAME}

      network.host: 0.0.0.0

      # see https://github.com/kubernetes/kubernetes/issues/3595
      bootstrap.memory_lock: ${BOOTSTRAP_MEMORY_LOCK:false}

      discovery:
        zen:
          ping.unicast.hosts: ${DISCOVERY_SERVICE}
          minimum_master_nodes: ${MINIMUM_MASTER_NODES:2}

      # see https://www.elastic.co/guide/en/x-pack/current/xpack-settings.html
      xpack.ml.enabled: ${XPACK_ML_ENABLED:false}
      xpack.monitoring.enabled: ${XPACK_MONITORING_ENABLED:false}
      xpack.security.enabled: ${XPACK_SECURITY_ENABLED:false}
      xpack.watcher.enabled: ${XPACK_WATCHER_ENABLED:false}

      # see https://github.com/elastic/elasticsearch-definitive-guide/pull/679
      processors: ${PROCESSORS:}

      # avoid split-brain w/ a minimum consensus of two masters plus a data node
      gateway.expected_master_nodes: ${EXPECTED_MASTER_NODES:2}
      gateway.expected_data_nodes: ${EXPECTED_DATA_NODES:1}
      gateway.recover_after_time: ${RECOVER_AFTER_TIME:5m}
      gateway.recover_after_master_nodes: ${RECOVER_AFTER_MASTER_NODES:2}
      gateway.recover_after_data_nodes: ${RECOVER_AFTER_DATA_NODES:1}
    logging.yml: |-
      # you can override this using by setting a system property, for example -Des.logger.level=DEBUG
      es.logger.level: INFO
      rootLogger: ${es.logger.level}, console
      logger:
        # log action execution errors for easier debugging
        action: DEBUG
        # reduce the logging for aws, too much is logged under the default INFO
        com.amazonaws: WARN

      appender:
        console:
          type: console
          layout:
            type: consolePattern
            conversionPattern: "[%d{ISO8601}][%-5p][%-25c] %m%n"
    pre-stop-hook.sh: |-
      #!/bin/bash
      set -e

      SERVICE_ACCOUNT_PATH=/var/run/secrets/kubernetes.io/serviceaccount
      KUBE_TOKEN=$(<${SERVICE_ACCOUNT_PATH}/token)
      KUBE_NAMESPACE=$(<${SERVICE_ACCOUNT_PATH}/namespace)

      STATEFULSET_NAME=$(echo "${HOSTNAME}" | sed 's/-[0-9]*$//g')
      INSTANCE_ID=$(echo "${HOSTNAME}" | grep -o '[0-9]*$')

      echo "Prepare stopping of Pet ${KUBE_NAMESPACE}/${HOSTNAME} of StatefulSet ${KUBE_NAMESPACE}/${STATEFULSET_NAME} instance_id ${INSTANCE_ID}"

      INSTANCES_DESIRED=$(curl -s \
        --cacert ${SERVICE_ACCOUNT_PATH}/ca.crt \
        -H "Authorization: Bearer $KUBE_TOKEN" \
        "https://${KUBERNETES_SERVICE_HOST}:${KUBERNETES_PORT_443_TCP_PORT}/apis/apps/v1beta1/namespaces/${KUBE_NAMESPACE}/statefulsets/${STATEFULSET_NAME}/status" | jq -r '.spec.replicas')

      echo "Desired instance count is ${INSTANCES_DESIRED}"

      if [ "${INSTANCE_ID}" -lt "${INSTANCES_DESIRED}" ]; then
        echo "No data migration needed"
        exit 0
      fi

      echo "Prepare to migrate data of the node"

      NODE_STATS=$(curl -s -XGET 'http://localhost:9200/_nodes/stats')
      NODE_IP=$(echo "${NODE_STATS}" | jq -r ".nodes[] | select(.name==\"${HOSTNAME}\") | .host")

      echo "Move all data from node ${NODE_IP}"

      curl -s -XPUT localhost:9200/_cluster/settings -d "{
        \"transient\" :{
            \"cluster.routing.allocation.exclude._ip\" : \"${NODE_IP}\"
        }
      }"
      echo

      echo "Wait for node to become empty"
      DOC_COUNT=$(echo "${NODE_STATS}" | jq ".nodes[] | select(.name==\"${HOSTNAME}\") | .indices.docs.count")
      while [ "${DOC_COUNT}" -gt 0 ]; do
        NODE_STATS=$(curl -s -XGET 'http://localhost:9200/_nodes/stats')
        DOC_COUNT=$(echo "${NODE_STATS}" | jq -r ".nodes[] | select(.name==\"${HOSTNAME}\") | .indices.docs.count")
        echo "Node contains ${DOC_COUNT} documents"
        sleep 1
      done

      echo "Wait for node shards to become empty"
      SHARD_STATS=$(curl -s -XGET 'http://localhost:9200/_cat/shards?format=json')
      SHARD_COUNT=$(echo "${SHARD_STATS}" | jq "[.[] | select(.node==\"${HOSTNAME}\")] | length")
      while [ "${SHARD_COUNT}" -gt 0 ]; do
        SHARD_STATS=$(curl -s -XGET 'http://localhost:9200/_cat/shards?format=json')
        SHARD_COUNT=$(echo "${SHARD_STATS}" | jq "[.[] | select(.node==\"${HOSTNAME}\")] | length")
        echo "Node contains ${SHARD_COUNT} shards"
        sleep 1
      done

      echo "Node clear to shutdown"










- apiVersion: v1
  kind: ServiceAccount
  metadata:
    labels:
      app: ${APPLICATION_NAME}
    name: elasticsearch

# Source: elasticsearch/templates/client-svc.yaml
- apiVersion: v1
  kind: Service
  metadata:
    labels:
      app: ${APPLICATION_NAME}
      component: elasticsearch
      role: client
    name: ${APPLICATION_NAME}-elasticsearch
  spec:
    ports:
      - name: http
        port: 9200
        targetPort: http
      - name: transport
        port: 9300
        targetPort: transport
    selector:
      app: ${APPLICATION_NAME}
      component: elasticsearch
      role: client
    type: ClusterIP




# Source: elasticsearch/templates/master-svc.yaml
- apiVersion: v1
  kind: Service
  metadata:
    labels:
      app: ${APPLICATION_NAME}
      component: elasticsearch
      role: master
    name: ${APPLICATION_NAME}-elasticsearch-discovery
  spec:
    clusterIP: None
    ports:
      - port: 9300
        targetPort: transport
    selector:
      app: ${APPLICATION_NAME}
      component: elasticsearch
      role: master



  # Source: elasticsearch/templates/client-deployment.yaml
- apiVersion: apps/v1beta1
  kind: Deployment
  metadata:
    labels:
      app: ${APPLICATION_NAME}
      component: elasticsearch
      role: client
    name: ${APPLICATION_NAME}-es-client
  spec:
    replicas: 2
    template:
      metadata:
        labels:
          app: ${APPLICATION_NAME}
          component: elasticsearch
          role: client
      spec:
        serviceAccountName: elasticsearch
        affinity:
          podAntiAffinity:
            preferredDuringSchedulingIgnoredDuringExecution:
            - weight: 1
              podAffinityTerm:
                topologyKey: kubernetes.io/hostname
                labelSelector:
                  matchLabels:
                    app: ${APPLICATION_NAME}
                    component: elasticsearch
                    role: client
        initContainers:
        # see https://www.elastic.co/guide/en/elasticsearch/reference/current/vm-max-map-count.html
        # and https://www.elastic.co/guide/en/elasticsearch/reference/current/setup-configuration-memory.html#mlockall
        - name: "sysctl"
          image: "busybox"
          imagePullPolicy: "Always"
          command: ["sysctl", "-w", "vm.max_map_count=262144"]
          securityContext:
            privileged: true
        containers:
        - name: elasticsearch
          env:
          - name: DISCOVERY_SERVICE
            value: ${APPLICATION_NAME}-elasticsearch-discovery
          - name: KUBERNETES_MASTER
            value: kubernetes.default.svc.cluster.local
          - name: KUBERNETES_NAMESPACE
            valueFrom:
              fieldRef:
                fieldPath: metadata.namespace
          - name: NODE_DATA
            value: "false"
          - name: NODE_INGEST
            value: "false"
          - name: NODE_MASTER
            value: "false"
          - name: PROCESSORS
            valueFrom:
              resourceFieldRef:
                resource: limits.cpu
          - name: ES_JAVA_OPTS
            value: "-Djava.net.preferIPv4Stack=true -Xms512m -Xmx512m"
          - name: MINIMUM_MASTER_NODES
            value: "2"
          resources:
              limits:
                cpu: "1"
              requests:
                cpu: 25m
                memory: 512Mi
              
          readinessProbe:
            httpGet:
              path: /_cluster/health?wait_for_status=yellow
              port: 9200
            initialDelaySeconds: 5
          livenessProbe:
            httpGet:
              path: /_cluster/health?wait_for_status=yellow
              port: 9200
            initialDelaySeconds: 90
          image: "docker.elastic.co/elasticsearch/elasticsearch:5.6.5"
          imagePullPolicy: "IfNotPresent"
          ports:
          - containerPort: 9200
            name: http
          - containerPort: 9300
            name: transport
          volumeMounts:
          - mountPath: /usr/share/elasticsearch/config/elasticsearch.yml
            name: config
            subPath: elasticsearch.yml
          - mountPath: /usr/share/elasticsearch/config/logging.yml
            name: config
            subPath: logging.yml
        volumes:
        - name: config
          configMap:
            name: ${APPLICATION_NAME}-elasticsearch-config


  # Source: elasticsearch/templates/data-statefulset.yaml
- apiVersion: apps/v1beta1
  kind: StatefulSet
  metadata:
    labels:
      app: ${APPLICATION_NAME}
      component: elasticsearch
      role: data
    name: ${APPLICATION_NAME}-elasticsearch-data
  spec:
    serviceName: ${APPLICATION_NAME}-elasticsearch-data
    replicas: 2
    template:
      metadata:
        labels:
          app: ${APPLICATION_NAME}
          component: elasticsearch
          role: data
      spec:
        serviceAccountName: elasticsearch
        affinity:
          podAntiAffinity:
            preferredDuringSchedulingIgnoredDuringExecution:
            - weight: 1
              podAffinityTerm:
                topologyKey: kubernetes.io/hostname
                labelSelector:
                  matchLabels:
                    app: ${APPLICATION_NAME}                    
                    component: elasticsearch
                    role: data
        initContainers:
        # see https://www.elastic.co/guide/en/elasticsearch/reference/current/vm-max-map-count.html
        # and https://www.elastic.co/guide/en/elasticsearch/reference/current/setup-configuration-memory.html#mlockall
        - name: "sysctl"
          image: "busybox"
          imagePullPolicy: "Always"
          command: ["sysctl", "-w", "vm.max_map_count=262144"]
          securityContext:
            privileged: true
        - name: "chown"
          image: "docker.elastic.co/elasticsearch/elasticsearch:5.6.5"
          imagePullPolicy: "IfNotPresent"
          command:
          - /bin/bash
          - -c
          - chown -R elasticsearch:elasticsearch /usr/share/elasticsearch/data &&
            chown -R elasticsearch:elasticsearch /usr/share/elasticsearch/logs
          securityContext:
            runAsUser: 0
          volumeMounts:
          - mountPath: /usr/share/elasticsearch/data
            name: data
        containers:
        - name: elasticsearch
          env:
          - name: DISCOVERY_SERVICE
            value: ${APPLICATION_NAME}-elasticsearch-discovery
          - name: KUBERNETES_MASTER
            value: kubernetes.default.svc.cluster.local
          - name: KUBERNETES_NAMESPACE
            valueFrom:
              fieldRef:
                fieldPath: metadata.namespace
          - name: NODE_MASTER
            value: "false"
          - name: PROCESSORS
            valueFrom:
              resourceFieldRef:
                resource: limits.cpu
          - name: ES_JAVA_OPTS
            value: "-Djava.net.preferIPv4Stack=true -Xms1536m -Xmx1536m"
          - name: MINIMUM_MASTER_NODES
            value: "2"
          image: "docker.elastic.co/elasticsearch/elasticsearch:5.6.5"
          imagePullPolicy: "IfNotPresent"
          ports:
          - containerPort: 9300
            name: transport
          resources:
              limits:
                cpu: "1"
              requests:
                cpu: 25m
                memory: 1536Mi
              
          readinessProbe:
            httpGet:
              path: /_cluster/health?local=true
              port: 9200
            initialDelaySeconds: 5
          volumeMounts:
          - mountPath: /usr/share/elasticsearch/data
            name: data
          - mountPath: /usr/share/elasticsearch/config/elasticsearch.yml
            name: config
            subPath: elasticsearch.yml
          - mountPath: /usr/share/elasticsearch/config/logging.yml
            name: config
            subPath: logging.yml
          - name: config
            mountPath: /pre-stop-hook.sh
            subPath: pre-stop-hook.sh
          lifecycle:
            preStop:
              exec:
                command: ["/bin/bash","/pre-stop-hook.sh"]
        terminationGracePeriodSeconds: 3600
        volumes:
        - name: config
          configMap:
            name: ${APPLICATION_NAME}-elasticsearch-config
    volumeClaimTemplates:
    - metadata:
        name: data
        labels:
          app: ${APPLICATION_NAME}   
        annotations:
          volume.beta.kubernetes.io/storage-class: aws-fast
      spec:
        accessModes:
          - "ReadWriteOnce"
        resources:
          requests:
            storage: ${VOLUME_ELASTICSEARCH_CAPACITY}


    

  # Source: elasticsearch/templates/master-statefulset.yaml
- apiVersion: apps/v1beta1
  kind: StatefulSet
  metadata:
    name: ${APPLICATION_NAME}-es-master
    labels:
      app: ${APPLICATION_NAME}
      component: elasticsearch
      role: master
  spec:
    serviceName: ${APPLICATION_NAME}-es-master
    replicas: 3
    template:
      metadata:
        labels:
          app: ${APPLICATION_NAME}
          component: elasticsearch
          role: master
      spec:
        serviceAccountName: elasticsearch
        affinity:
          podAntiAffinity:
            preferredDuringSchedulingIgnoredDuringExecution:
            - weight: 1
              podAffinityTerm:
                topologyKey: kubernetes.io/hostname
                labelSelector:
                  matchLabels:
                    app: ${APPLICATION_NAME}
                    component: elasticsearch
                    role: master
        initContainers:
        # see https://www.elastic.co/guide/en/elasticsearch/reference/current/vm-max-map-count.html
        # and https://www.elastic.co/guide/en/elasticsearch/reference/current/setup-configuration-memory.html#mlockall
        - name: "sysctl"
          image: "busybox"
          imagePullPolicy: "Always"
          command: ["sysctl", "-w", "vm.max_map_count=262144"]
          securityContext:
            privileged: true
        - name: "chown"
          image: "docker.elastic.co/elasticsearch/elasticsearch:5.6.5"
          imagePullPolicy: "IfNotPresent"
          command:
          - /bin/bash
          - -c
          - chown -R elasticsearch:elasticsearch /usr/share/elasticsearch/data &&
            chown -R elasticsearch:elasticsearch /usr/share/elasticsearch/logs
          securityContext:
            runAsUser: 0
          volumeMounts:
          - mountPath: /usr/share/elasticsearch/data
            name: data
        containers:
        - name: elasticsearch
          env:
          - name: DISCOVERY_SERVICE
            value: ${APPLICATION_NAME}-elasticsearch-discovery
          - name: KUBERNETES_MASTER
            value: kubernetes.default.svc.cluster.local
          - name: KUBERNETES_NAMESPACE
            valueFrom:
              fieldRef:
                fieldPath: metadata.namespace
          - name: NODE_DATA
            value: "false"
          - name: NODE_INGEST
            value: "false"
          - name: PROCESSORS
            valueFrom:
              resourceFieldRef:
                resource: limits.cpu
          - name: ES_JAVA_OPTS
            value: "-Djava.net.preferIPv4Stack=true -Xms512m -Xmx512m"
          - name: MINIMUM_MASTER_NODES
            value: "2"
          resources:
              limits:
                cpu: "1"
              requests:
                cpu: 25m
                memory: 512Mi
              
          readinessProbe:
            httpGet:
              path: /_cluster/health?local=true
              port: 9200
            initialDelaySeconds: 5
          image: "docker.elastic.co/elasticsearch/elasticsearch:5.6.5"
          imagePullPolicy: "IfNotPresent"
          ports:
          - containerPort: 9300
            name: transport
          volumeMounts:
          - mountPath: /usr/share/elasticsearch/data
            name: data
          - mountPath: /usr/share/elasticsearch/config/elasticsearch.yml
            name: config
            subPath: elasticsearch.yml
          - mountPath: /usr/share/elasticsearch/config/logging.yml
            name: config
            subPath: logging.yml
        volumes:
        - name: config
          configMap:
            name: ${APPLICATION_NAME}-elasticsearch-config
    volumeClaimTemplates:
    - metadata:
        name: data
        labels:
          app: ${APPLICATION_NAME}   
        annotations:
          volume.beta.kubernetes.io/storage-class: aws-fast
      spec:
        accessModes:
          - "ReadWriteOnce"
        resources:
          requests:
            storage: "4Gi"




- apiVersion: v1
  kind: Service
  metadata:
    name: ${APPLICATION_NAME}-mongo
    labels:
      app: ${APPLICATION_NAME}
      component: mongo
  spec:
    ports:
    - port: 27017
      targetPort: 27017
    clusterIP: None
    selector:
      app: ${APPLICATION_NAME}
      component: mongo

- apiVersion: apps/v1beta1
  kind: StatefulSet
  metadata:
    name: ${APPLICATION_NAME}-mongo
  spec:
    serviceName: ${APPLICATION_NAME}-mongo
    replicas: 3
    template:
      metadata:
        labels:
          app: ${APPLICATION_NAME}
          component: mongo          
      spec:
        terminationGracePeriodSeconds: 10
        containers:
          - name: mongo
            image: mongo:3.4
            command:
              - mongod
              - "--replSet"
              - ${MONGODB_REPLICASET_NAME}
              - "--smallfiles"
              - "--noprealloc"
            ports:
              - containerPort: 27017
            volumeMounts:
              - name: data
                mountPath: /data/db
          - name: mongo-sidecar
            image: cvallance/mongo-k8s-sidecar
            env:
              - name: MONGO_SIDECAR_POD_LABELS
                value: "app=${APPLICATION_NAME},component=mongo"
              - name: KUBERNETES_MONGO_SERVICE_NAME
                value: ${APPLICATION_NAME}-mongo
              - name: KUBE_NAMESPACE
                valueFrom:
                  fieldRef:
                    fieldPath: metadata.namespace

    volumeClaimTemplates:
    - metadata:
        name: data
        labels:
          app: ${APPLICATION_NAME}   
        annotations:
          volume.beta.kubernetes.io/storage-class: aws-fast
      spec:
        accessModes: [ "ReadWriteOnce" ]
        resources:
          requests:
            # TODO: add parameter
            storage: ${VOLUME_MONGODB_CAPACITY}

- kind: ConfigMap
  metadata:
    name: ${APPLICATION_NAME}-zookeeper-config  
    labels:    
      app: ${APPLICATION_NAME}
      component: zookeeper
      role: config
  apiVersion: v1
  data:
    init.sh: |-
      #!/bin/bash
      set -x

      [ -z "$ID_OFFSET" ] && ID_OFFSET=1
      export ZOOKEEPER_SERVER_ID=$((${HOSTNAME##*-} + $ID_OFFSET))
      echo "${ZOOKEEPER_SERVER_ID:-1}" | tee /var/lib/zookeeper/data/myid
      sed -i "s/server\.$ZOOKEEPER_SERVER_ID\=[a-z0-9.-]*/server.$ZOOKEEPER_SERVER_ID=0.0.0.0/" /etc/kafka/zookeeper.properties

    zookeeper.properties: |-
      tickTime=2000
      dataDir=/var/lib/zookeeper/data
      dataLogDir=/var/lib/zookeeper/log
      clientPort=2181
      initLimit=5
      syncLimit=2
      server.1=${APPLICATION_NAME}-pzoo-0.${APPLICATION_NAME}-pzoo:2888:3888:participant
      server.2=${APPLICATION_NAME}-pzoo-1.${APPLICATION_NAME}-pzoo:2888:3888:participant
      server.3=${APPLICATION_NAME}-pzoo-2.${APPLICATION_NAME}-pzoo:2888:3888:participant
      server.4=${APPLICATION_NAME}-zoo-0.${APPLICATION_NAME}-zoo:2888:3888:participant
      server.5=${APPLICATION_NAME}-zoo-1.${APPLICATION_NAME}-zoo:2888:3888:participant

    log4j.properties: |-
      log4j.rootLogger=INFO, stdout
      log4j.appender.stdout=org.apache.log4j.ConsoleAppender
      log4j.appender.stdout.layout=org.apache.log4j.PatternLayout
      log4j.appender.stdout.layout.ConversionPattern=[%d] %p %m (%c)%n

      # Suppress connection log messages, three lines per livenessProbe execution
      log4j.logger.org.apache.zookeeper.server.NIOServerCnxnFactory=WARN
      log4j.logger.org.apache.zookeeper.server.NIOServerCnxn=WARN



- apiVersion: v1
  kind: Service
  metadata:
    name: ${APPLICATION_NAME}-pzoo
    labels:    
      app: ${APPLICATION_NAME}
      component: zookeeper
      role: service
  spec:
    ports:
    - port: 2888
      name: peer
    - port: 3888
      name: leader-election
    clusterIP: None
    selector:
      app: ${APPLICATION_NAME}
      component: zookeeper
      storage: persistent

- apiVersion: v1
  kind: Service
  metadata:
    name: ${APPLICATION_NAME}-zoo
    labels:
      app: ${APPLICATION_NAME}
      component: zookeeper
      role: service
  spec:
    ports:
    - port: 2888
      name: peer
    - port: 3888
      name: leader-election
    clusterIP: None
    selector:
      app: ${APPLICATION_NAME}
      component: zookeeper
      storage: ephemeral

# the headless service is for PetSet DNS, this one is for clients
- apiVersion: v1
  kind: Service
  metadata:
    name: ${APPLICATION_NAME}-zookeeper
    labels:    
      app: ${APPLICATION_NAME}
      component: zookeper-client
      role: service-client
  spec:
    ports:
    - port: 2181
      name: client
    selector:
      app: ${APPLICATION_NAME}
      component: zookeeper

- apiVersion: apps/v1beta1
  kind: StatefulSet
  metadata:
    name: ${APPLICATION_NAME}-pzoo
  spec:
    serviceName: "${APPLICATION_NAME}-pzoo"
    replicas: 3
    template:
      metadata:
        labels:
          app: ${APPLICATION_NAME}
          component: zookeeper
          role: pdata
          storage: persistent
        annotations:
      spec:
        terminationGracePeriodSeconds: 10
        initContainers:
        - name: init-config
          image: solsson/kafka:1.0.0
          command: ['/bin/bash', '/etc/kafka/init.sh']
          volumeMounts:
          - name: config
            mountPath: /etc/kafka
          - name: data-${APPLICATION_NAME}
            mountPath: /var/lib/zookeeper/data
        containers:
        - name: zookeeper
          image: solsson/kafka:1.0.0
          env:
          - name: KAFKA_LOG4J_OPTS
            value: -Dlog4j.configuration=file:/etc/kafka/log4j.properties
          command:
          - ./bin/zookeeper-server-start.sh
          - /etc/kafka/zookeeper.properties
          ports:
          - containerPort: 2181
            name: client
          - containerPort: 2888
            name: peer
          - containerPort: 3888
            name: leader-election
          resources:
            requests:
              cpu: 10m
              memory: 100Mi
          readinessProbe:
            exec:
              command:
              - /bin/sh
              - -c
              - '[ "imok" = "$(echo ruok | nc -w 1 127.0.0.1 2181)" ]'
          volumeMounts:
          - name: config
            mountPath: /etc/kafka
          - name: data-${APPLICATION_NAME}
            mountPath: /var/lib/zookeeper/data
          - name: log
            mountPath: /var/lib/zookeeper/log
        volumes:
        - name: config
          configMap:
            name: ${APPLICATION_NAME}-zookeeper-config
        - name: log
          emptyDir: {}
    volumeClaimTemplates:
    - metadata:
        name: data-${APPLICATION_NAME}
        labels:
          app: ${APPLICATION_NAME} 
          component: zookeper
        annotations:
          volume.beta.kubernetes.io/storage-class: aws-fast
      spec:
        accessModes: [ "ReadWriteOnce" ]
        resources:
          requests:
            storage: ${VOLUME_ZOOKEEPER_CAPACITY}


- apiVersion: apps/v1beta1
  kind: StatefulSet
  metadata:
    name: ${APPLICATION_NAME}-zoo
  spec:
    serviceName: "${APPLICATION_NAME}-zoo"
    replicas: 2
    template:
      metadata:
        labels:
          app: ${APPLICATION_NAME}
          component: zookeeper
          role: edata
          storage: ephemeral
        annotations:
      spec:
        terminationGracePeriodSeconds: 10
        initContainers:
        - name: init-config
          image: solsson/kafka:1.0.0
          command: ['/bin/bash', '/etc/kafka/init.sh']
          env:
          - name: ID_OFFSET
            value: "4"
          volumeMounts:
          - name: config
            mountPath: /etc/kafka
          - name: data
            mountPath: /var/lib/zookeeper/data
        containers:
        - name: zookeeper
          image: solsson/kafka:1.0.0
          env:
          - name: KAFKA_LOG4J_OPTS
            value: -Dlog4j.configuration=file:/etc/kafka/log4j.properties
          command:
          - ./bin/zookeeper-server-start.sh
          - /etc/kafka/zookeeper.properties
          ports:
          - containerPort: 2181
            name: client
          - containerPort: 2888
            name: peer
          - containerPort: 3888
            name: leader-election
          resources:
            requests:
              cpu: 10m
              memory: 100Mi
          readinessProbe:
            exec:
              command:
              - /bin/sh
              - -c
              - '[ "imok" = "$(echo ruok | nc -w 1 127.0.0.1 2181)" ]'
          volumeMounts:
          - name: config
            mountPath: /etc/kafka
          - name: data
            mountPath: /var/lib/zookeeper/data
          - name: log
            mountPath: /var/lib/zookeeper/log
        volumes:
        - name: config
          configMap:
            name: ${APPLICATION_NAME}-zookeeper-config
        - name: data
          emptyDir: {}
        - name: log
          emptyDir: {}


- kind: ConfigMap
  metadata:
    name: ${APPLICATION_NAME}-broker-config
    labels:
          app: ${APPLICATION_NAME}
          component: kafka
          role: config
  apiVersion: v1
  data:
    init.sh: |-
      #!/bin/bash
      set -x

      KAFKA_BROKER_ID=${HOSTNAME##*-}
      sed -i "s/#init#broker.id=#init#/broker.id=$KAFKA_BROKER_ID/" /etc/kafka/server.properties

      hash kubectl 2>/dev/null || {
        sed -i "s/#init#broker.rack=#init#/#init#broker.rack=# kubectl not found in path/" /etc/kafka/server.properties
      } && {
        ZONE=$(kubectl get node "$NODE_NAME" -o=go-template='{{index .metadata.labels "failure-domain.beta.kubernetes.io/zone"}}')
        if [ $? -ne 0 ]; then
          sed -i "s/#init#broker.rack=#init#/#init#broker.rack=# zone lookup failed, see -c init-config logs/" /etc/kafka/server.properties
        elif [ "x$ZONE" == "x<no value>" ]; then
          sed -i "s/#init#broker.rack=#init#/#init#broker.rack=# zone label not found for node $NODE_NAME/" /etc/kafka/server.properties
        else
          sed -i "s/#init#broker.rack=#init#/broker.rack=$ZONE/" /etc/kafka/server.properties
        fi
        
        NODEIP=$(kubectl get node "$NODE_NAME" -o jsonpath='{.status.addresses[?(@.type=="InternalIP")].address}')
        sed -i "s/#init#nodeip#/$NODEIP/" /etc/kafka/server.properties
      }

    server.properties: |-
      # Licensed to the Apache Software Foundation (ASF) under one or more
      # contributor license agreements.  See the NOTICE file distributed with
      # this work for additional information regarding copyright ownership.
      # The ASF licenses this file to You under the Apache License, Version 2.0
      # (the "License"); you may not use this file except in compliance with
      # the License.  You may obtain a copy of the License at
      #
      #    http://www.apache.org/licenses/LICENSE-2.0
      #
      # Unless required by applicable law or agreed to in writing, software
      # distributed under the License is distributed on an "AS IS" BASIS,
      # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
      # See the License for the specific language governing permissions and
      # limitations under the License.

      # see kafka.server.KafkaConfig for additional details and defaults

      ############################# Server Basics #############################

      # The id of the broker. This must be set to a unique integer for each broker.
      #init#broker.id=#init#

      #init#broker.rack=#init#

      # Switch to enable topic deletion or not, default value is false
      delete.topic.enable=true

      ############################# Socket Server Settings #############################

      # The address the socket server listens on. It will get the value returned from
      # java.net.InetAddress.getCanonicalHostName() if not configured.
      #   FORMAT:
      #     listeners = listener_name://host_name:port
      #   EXAMPLE:
      #     listeners = PLAINTEXT://your.host.name:9092
      #listeners=PLAINTEXT://:9092

      listener.security.protocol.map=CLIENT:PLAINTEXT,REPLICATION:PLAINTEXT
      advertised.listeners=CLIENT://:9092,REPLICATION://#init#nodeip#:32094
      listeners=CLIENT://:9092,REPLICATION://:9094
      inter.broker.listener.name=CLIENT

      # Hostname and port the broker will advertise to producers and consumers. If not set,
      # it uses the value for "listeners" if configured.  Otherwise, it will use the value
      # returned from java.net.InetAddress.getCanonicalHostName().
      #advertised.listeners=PLAINTEXT://your.host.name:9092

      # Maps listener names to security protocols, the default is for them to be the same. See the config documentation for more details
      #listener.security.protocol.map=PLAINTEXT:PLAINTEXT,SSL:SSL,SASL_PLAINTEXT:SASL_PLAINTEXT,SASL_SSL:SASL_SSL

      # The number of threads that the server uses for receiving requests from the network and sending responses to the network
      num.network.threads=3

      # The number of threads that the server uses for processing requests, which may include disk I/O
      num.io.threads=8

      # The send buffer (SO_SNDBUF) used by the socket server
      socket.send.buffer.bytes=102400

      # The receive buffer (SO_RCVBUF) used by the socket server
      socket.receive.buffer.bytes=102400

      # The maximum size of a request that the socket server will accept (protection against OOM)
      socket.request.max.bytes=104857600


      ############################# Log Basics #############################

      # A comma seperated list of directories under which to store log files
      log.dirs=/tmp/kafka-logs

      # The default number of log partitions per topic. More partitions allow greater
      # parallelism for consumption, but this will also result in more files across
      # the brokers.
      num.partitions=1

      # The number of threads per data directory to be used for log recovery at startup and flushing at shutdown.
      # This value is recommended to be increased for installations with data dirs located in RAID array.
      num.recovery.threads.per.data.dir=1

      ############################# Internal Topic Settings  #############################
      # The replication factor for the group metadata internal topics "__consumer_offsets" and "__transaction_state"
      # For anything other than development testing, a value greater than 1 is recommended for to ensure availability such as 3.
      offsets.topic.replication.factor=1
      transaction.state.log.replication.factor=1
      transaction.state.log.min.isr=1

      ############################# Log Flush Policy #############################

      # Messages are immediately written to the filesystem but by default we only fsync() to sync
      # the OS cache lazily. The following configurations control the flush of data to disk.
      # There are a few important trade-offs here:
      #    1. Durability: Unflushed data may be lost if you are not using replication.
      #    2. Latency: Very large flush intervals may lead to latency spikes when the flush does occur as there will be a lot of data to flush.
      #    3. Throughput: The flush is generally the most expensive operation, and a small flush interval may lead to exceessive seeks.
      # The settings below allow one to configure the flush policy to flush data after a period of time or
      # every N messages (or both). This can be done globally and overridden on a per-topic basis.

      # The number of messages to accept before forcing a flush of data to disk
      #log.flush.interval.messages=10000

      # The maximum amount of time a message can sit in a log before we force a flush
      #log.flush.interval.ms=1000

      ############################# Log Retention Policy #############################

      # The following configurations control the disposal of log segments. The policy can
      # be set to delete segments after a period of time, or after a given size has accumulated.
      # A segment will be deleted whenever *either* of these criteria are met. Deletion always happens
      # from the end of the log.

      # The minimum age of a log file to be eligible for deletion due to age
      log.retention.hours=168

      # A size-based retention policy for logs. Segments are pruned from the log as long as the remaining
      # segments don't drop below log.retention.bytes. Functions independently of log.retention.hours.
      #log.retention.bytes=1073741824

      # The maximum size of a log segment file. When this size is reached a new log segment will be created.
      log.segment.bytes=1073741824

      # The interval at which log segments are checked to see if they can be deleted according
      # to the retention policies
      log.retention.check.interval.ms=300000

      ############################# Zookeeper #############################

      # Zookeeper connection string (see zookeeper docs for details).
      # This is a comma separated host:port pairs, each corresponding to a zk
      # server. e.g. "127.0.0.1:3000,127.0.0.1:3001,127.0.0.1:3002".
      # You can also append an optional chroot string to the urls to specify the
      # root directory for all kafka znodes.
      zookeeper.connect=localhost:2181

      # Timeout in ms for connecting to zookeeper
      zookeeper.connection.timeout.ms=6000


      ############################# Group Coordinator Settings #############################

      # The following configuration specifies the time, in milliseconds, that the GroupCoordinator will delay the initial consumer rebalance.
      # The rebalance will be further delayed by the value of group.initial.rebalance.delay.ms as new members join the group, up to a maximum of max.poll.interval.ms.
      # The default value for this is 3 seconds.
      # We override this to 0 here as it makes for a better out-of-the-box experience for development and testing.
      # However, in production environments the default value of 3 seconds is more suitable as this will help to avoid unnecessary, and potentially expensive, rebalances during application startup.
      group.initial.rebalance.delay.ms=3000

    log4j.properties: |-
      # Licensed to the Apache Software Foundation (ASF) under one or more
      # contributor license agreements.  See the NOTICE file distributed with
      # this work for additional information regarding copyright ownership.
      # The ASF licenses this file to You under the Apache License, Version 2.0
      # (the "License"); you may not use this file except in compliance with
      # the License.  You may obtain a copy of the License at
      #
      #    http://www.apache.org/licenses/LICENSE-2.0
      #
      # Unless required by applicable law or agreed to in writing, software
      # distributed under the License is distributed on an "AS IS" BASIS,
      # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
      # See the License for the specific language governing permissions and
      # limitations under the License.

      # Unspecified loggers and loggers with additivity=true output to server.log and stdout
      # Note that INFO only applies to unspecified loggers, the log level of the child logger is used otherwise
      log4j.rootLogger=INFO, stdout

      log4j.appender.stdout=org.apache.log4j.ConsoleAppender
      log4j.appender.stdout.layout=org.apache.log4j.PatternLayout
      log4j.appender.stdout.layout.ConversionPattern=[%d] %p %m (%c)%n

      log4j.appender.kafkaAppender=org.apache.log4j.DailyRollingFileAppender
      log4j.appender.kafkaAppender.DatePattern='.'yyyy-MM-dd-HH
      log4j.appender.kafkaAppender.File=${kafka.logs.dir}/server.log
      log4j.appender.kafkaAppender.layout=org.apache.log4j.PatternLayout
      log4j.appender.kafkaAppender.layout.ConversionPattern=[%d] %p %m (%c)%n

      log4j.appender.stateChangeAppender=org.apache.log4j.DailyRollingFileAppender
      log4j.appender.stateChangeAppender.DatePattern='.'yyyy-MM-dd-HH
      log4j.appender.stateChangeAppender.File=${kafka.logs.dir}/state-change.log
      log4j.appender.stateChangeAppender.layout=org.apache.log4j.PatternLayout
      log4j.appender.stateChangeAppender.layout.ConversionPattern=[%d] %p %m (%c)%n

      log4j.appender.requestAppender=org.apache.log4j.DailyRollingFileAppender
      log4j.appender.requestAppender.DatePattern='.'yyyy-MM-dd-HH
      log4j.appender.requestAppender.File=${kafka.logs.dir}/kafka-request.log
      log4j.appender.requestAppender.layout=org.apache.log4j.PatternLayout
      log4j.appender.requestAppender.layout.ConversionPattern=[%d] %p %m (%c)%n

      log4j.appender.cleanerAppender=org.apache.log4j.DailyRollingFileAppender
      log4j.appender.cleanerAppender.DatePattern='.'yyyy-MM-dd-HH
      log4j.appender.cleanerAppender.File=${kafka.logs.dir}/log-cleaner.log
      log4j.appender.cleanerAppender.layout=org.apache.log4j.PatternLayout
      log4j.appender.cleanerAppender.layout.ConversionPattern=[%d] %p %m (%c)%n

      log4j.appender.controllerAppender=org.apache.log4j.DailyRollingFileAppender
      log4j.appender.controllerAppender.DatePattern='.'yyyy-MM-dd-HH
      log4j.appender.controllerAppender.File=${kafka.logs.dir}/controller.log
      log4j.appender.controllerAppender.layout=org.apache.log4j.PatternLayout
      log4j.appender.controllerAppender.layout.ConversionPattern=[%d] %p %m (%c)%n

      log4j.appender.authorizerAppender=org.apache.log4j.DailyRollingFileAppender
      log4j.appender.authorizerAppender.DatePattern='.'yyyy-MM-dd-HH
      log4j.appender.authorizerAppender.File=${kafka.logs.dir}/kafka-authorizer.log
      log4j.appender.authorizerAppender.layout=org.apache.log4j.PatternLayout
      log4j.appender.authorizerAppender.layout.ConversionPattern=[%d] %p %m (%c)%n

      # Change the two lines below to adjust ZK client logging
      log4j.logger.org.I0Itec.zkclient.ZkClient=INFO
      log4j.logger.org.apache.zookeeper=INFO

      # Change the two lines below to adjust the general broker logging level (output to server.log and stdout)
      log4j.logger.kafka=INFO
      log4j.logger.org.apache.kafka=INFO

      # Change to DEBUG or TRACE to enable request logging
      log4j.logger.kafka.request.logger=WARN, requestAppender
      log4j.additivity.kafka.request.logger=false

      # Uncomment the lines below and change log4j.logger.kafka.network.RequestChannel$ to TRACE for additional output
      # related to the handling of requests
      #log4j.logger.kafka.network.Processor=TRACE, requestAppender
      #log4j.logger.kafka.server.KafkaApis=TRACE, requestAppender
      #log4j.additivity.kafka.server.KafkaApis=false
      log4j.logger.kafka.network.RequestChannel$=WARN, requestAppender
      log4j.additivity.kafka.network.RequestChannel$=false

      log4j.logger.kafka.controller=TRACE, controllerAppender
      log4j.additivity.kafka.controller=false

      log4j.logger.kafka.log.LogCleaner=INFO, cleanerAppender
      log4j.additivity.kafka.log.LogCleaner=false

      log4j.logger.state.change.logger=TRACE, stateChangeAppender
      log4j.additivity.state.change.logger=false

      # Change to DEBUG to enable audit log for the authorizer
      log4j.logger.kafka.authorizer.logger=WARN, authorizerAppender
      log4j.additivity.kafka.authorizer.logger=false


- apiVersion: v1
  kind: Service
  metadata:
    name: ${APPLICATION_NAME}-kafka
    labels:
      app: ${APPLICATION_NAME}
      component: kafka
  spec:
    ports:
    - port: 9092
    # [podname].broker.kafka.svc.cluster.local
    clusterIP: None
    selector:
      app: ${APPLICATION_NAME}
      component: kafka

- apiVersion: apps/v1beta1
  kind: StatefulSet
  metadata:
    name: ${APPLICATION_NAME}-kafka
    labels:
      app: ${APPLICATION_NAME}
      component: kafka
  spec:
    serviceName: "${APPLICATION_NAME}-kafka"
    replicas: 3
    template:
      metadata:
        labels:
          app: ${APPLICATION_NAME}
          component: kafka
          role: data
        annotations:
      spec:
        terminationGracePeriodSeconds: 30
        initContainers:
        - name: init-config
          image: solsson/kafka-initutils@sha256:c275d681019a0d8f01295dbd4a5bae3cfa945c8d0f7f685ae1f00f2579f08c7d
          env:
          - name: NODE_NAME
            valueFrom:
              fieldRef:
                fieldPath: spec.nodeName
          command: ['/bin/bash', '/etc/kafka/init.sh']
          volumeMounts:
          - name: config
            mountPath: /etc/kafka
        containers:
        - name: broker
          image: solsson/kafka:1.0.0
          env:
          - name: KAFKA_LOG4J_OPTS
            value: -Dlog4j.configuration=file:/etc/kafka/log4j.properties
          ports:
          - containerPort: 9092
          command:
          - ./bin/kafka-server-start.sh
          - /etc/kafka/server.properties
          - --override
          -   zookeeper.connect=${APPLICATION_NAME}-zookeeper:2181
          - --override
          -   log.retention.hours=-1
          - --override
          -   log.dirs=/var/lib/kafka/data/topics
          - --override
          -   auto.create.topics.enable=true
          resources:
            requests:
              cpu: 100m
              memory: 512Mi
          readinessProbe:
            exec:
              command:
              - /bin/sh
              - -c
              - 'echo "" | nc -w 1 127.0.0.1 9092'
          volumeMounts:
          - name: config
            mountPath: /etc/kafka
          - name: data-${APPLICATION_NAME}
            mountPath: /var/lib/kafka/data
        volumes:
        - name: config
          configMap:
            name: ${APPLICATION_NAME}-broker-config
    volumeClaimTemplates:
    - metadata:
        name: data-${APPLICATION_NAME}
        labels:
          app: ${APPLICATION_NAME}
          component: kafka
      annotations:
        volume.beta.kubernetes.io/storage-class: aws-fast
      spec:
        accessModes: [ "ReadWriteOnce" ]
        resources:
          requests:
            storage: ${VOLUME_KAFKA_CAPACITY}


