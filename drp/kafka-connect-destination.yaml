kind: ConfigMap
metadata:
  name: mongodb-connect-config
  labels:    
    app: mirroring
    component: mongodb-connect
    role: config
apiVersion: v1
data:
  mongodb-connect.properties: |-
    name=mongodb-oplog
    connector.class=MongodbSink
    tasks.max=1
    uri=mongodb://nxpassive-mongo-0.nxpassive-mongo:27017,nxpassive-mongo-1.nxpassive-mongo:27017,nxpassive-mongo-2.nxpassive-mongo:27017
    bulk.size=100
    mongodb.database=nuxeo
    mongodb.collections=default
    converter.class=org.apache.kafka.connect.mongodb.converter.JsonStructConverter
    topics=nxmirror-mongo_nuxeo_default

    ##Host and port are mandatory but never used when uri is specified
    host=nxpassive-mongo-0.nxpassive-mongo
    port=27017
    

  connect.properties: |-
    ##
    # Licensed to the Apache Software Foundation (ASF) under one or more
    # contributor license agreements.  See the NOTICE file distributed with
    # this work for additional information regarding copyright ownership.
    # The ASF licenses this file to You under the Apache License, Version 2.0
    # (the "License"); you may not use this file except in compliance with
    # the License.  You may obtain a copy of the License at
    #
    #    http://www.apache.org/licenses/LICENSE-2.0
    #
    # Unless required by applicable law or agreed to in writing, software
    # distributed under the License is distributed on an "AS IS" BASIS,
    # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
    # See the License for the specific language governing permissions and
    # limitations under the License.
    ##

    # This file contains some of the configurations for the Kafka Connect distributed worker. This file is intended
    # to be used with the examples, and some settings may differ from those used in a production system, especially
    # the `bootstrap.servers` and those specifying replication factors.

    # A list of host/port pairs to use for establishing the initial connection to the Kafka cluster.
    bootstrap.servers=nxpassive-kafka-0.nxpassive-kafka:9092,nxpassive-kafka-1.nxpassive-kafka:9092,nxpassive-kafka-2.nxpassive-kafka:9092

    # unique name for the cluster, used in forming the Connect cluster group. Note that this must not conflict with consumer group IDs
    group.id=connect-mongodb-cluster

    # The converters specify the format of data in Kafka and how to translate it into Connect data. Every Connect user will
    # need to configure these based on the format they want their data in when loaded from or stored into Kafka
    key.converter=org.apache.kafka.connect.json.JsonConverter
    value.converter=org.apache.kafka.connect.json.JsonConverter
    # Converter-specific settings can be passed in by prefixing the Converter's setting with the converter we want to apply
    # it to
    key.converter.schemas.enable=true
    value.converter.schemas.enable=true

    # The internal converter used for offsets, config, and status data is configurable and must be specified, but most users will
    # always want to use the built-in default. Offset, config, and status data is never visible outside of Kafka Connect in this format.
    internal.key.converter=org.apache.kafka.connect.json.JsonConverter
    internal.value.converter=org.apache.kafka.connect.json.JsonConverter
    internal.key.converter.schemas.enable=false
    internal.value.converter.schemas.enable=false

    # Topic to use for storing offsets. This topic should have many partitions and be replicated and compacted.
    # Kafka Connect will attempt to create the topic automatically when needed, but you can always manually create
    # the topic before starting Kafka Connect if a specific topic configuration is needed.
    # Most users will want to use the built-in default replication factor of 3 or in some cases even specify a larger value.
    # Since this means there must be at least as many brokers as the maximum replication factor used, we'd like to be able
    # to run this example on a single-broker cluster and so here we instead set the replication factor to 1.
    offset.storage.topic=connect-mongodb-offsets
    offset.storage.replication.factor=1
    #offset.storage.partitions=25

    # Topic to use for storing connector and task configurations; note that this should be a single partition, highly replicated,
    # and compacted topic. Kafka Connect will attempt to create the topic automatically when needed, but you can always manually create
    # the topic before starting Kafka Connect if a specific topic configuration is needed.
    # Most users will want to use the built-in default replication factor of 3 or in some cases even specify a larger value.
    # Since this means there must be at least as many brokers as the maximum replication factor used, we'd like to be able
    # to run this example on a single-broker cluster and so here we instead set the replication factor to 1.
    config.storage.topic=connect-mongodb-configs
    config.storage.replication.factor=1

    # Topic to use for storing statuses. This topic can have multiple partitions and should be replicated and compacted.
    # Kafka Connect will attempt to create the topic automatically when needed, but you can always manually create
    # the topic before starting Kafka Connect if a specific topic configuration is needed.
    # Most users will want to use the built-in default replication factor of 3 or in some cases even specify a larger value.
    # Since this means there must be at least as many brokers as the maximum replication factor used, we'd like to be able
    # to run this example on a single-broker cluster and so here we instead set the replication factor to 1.
    status.storage.topic=connect-mongodb-status
    status.storage.replication.factor=1
    #status.storage.partitions=5

    # Flush much faster than normal, which is useful for testing/debugging
    offset.flush.interval.ms=10000

    # These are provided to inform the user about the presence of the REST host and port configs
    # Hostname & Port for the REST API to listen on. If this is set, it will bind to the interface used to listen to requests.
    #rest.host.name=
    #rest.port=8083

    # The Hostname & Port that will be given out to other workers to connect to i.e. URLs that are routable from other servers.
    #rest.advertised.host.name=
    #rest.advertised.port=

    # Set to a list of filesystem paths separated by commas (,) to enable class loading isolation for plugins
    # (connectors, converters, transformations). The list should consist of top level directories that include
    # any combination of:
    # a) directories immediately containing jars with plugins and their dependencies
    # b) uber-jars with plugins and their dependencies
    # c) directories immediately containing the package directory structure of classes of plugins and their dependencies
    # Examples:
    # plugin.path=/usr/local/share/java,/usr/local/share/kafka/plugins,/opt/connectors,
    plugin.path=/plugins
  log4j.properties: |-
    log4j.rootLogger=INFO, stdout
    log4j.appender.stdout=org.apache.log4j.ConsoleAppender
    log4j.appender.stdout.layout=org.apache.log4j.PatternLayout
    log4j.appender.stdout.layout.ConversionPattern=[%d] %p %m (%c)%n

    # Suppress connection log messages, three lines per livenessProbe execution
    log4j.logger.org.apache.zookeeper.server.NIOServerCnxnFactory=WARN
    log4j.logger.org.apache.zookeeper.server.NIOServerCnxn=WARN


---

apiVersion: v1
kind: DeploymentConfig
metadata:
  name: mongodb-connect
  labels:
    app: mirroring
    component: mirrormaker
spec:
  replicas: 1
  template:
    metadata:
      labels:
        app: mirroring
        component: mongodb-connect
    spec:      
      containers:
      - name: mongodb-connect
        image: docker-registry.default.svc:5000/nuxeo/mongodb-connect:latest
        env:
          - name: KAFKA_LOG4J_OPTS
            value: -Dlog4j.configuration=file:/etc/kafka/log4j.properties
        ports:
          - containerPort: 8083
        command:
          - ./bin/connect-distributed.sh
          -   /etc/kafka/connect.properties
          -   /etc/kafka/mongodb-connect.properties
        resources:
          requests:
            cpu: 100m
            memory: 512Mi
        volumeMounts:
        - name: config
          mountPath: /etc/kafka          
        - name: logs
          mountPath: /opt/kafka/logs
      volumes:
      - name: config
        configMap:
          name: mongodb-connect-config
      - name: logs
        emptyDir: {}

---

apiVersion: v1
kind: Service
metadata:
  name: mongodb-connect
  labels:
    app: mirroring
spec:
  type: LoadBalancer
  selector:
    app: mirroring
    component: mongodb-connect
  ports:
  - name: 8083-tcp
    port: 8083
    protocol: TCP

---
apiVersion: v1
kind: Route
metadata:
  labels:
    app: mirroring
  name: mongodb-connect-route  
spec:
  # TODO: Add parameter
  host: mongodb-passive.apps.io.nuxeo.com
  to:
    kind: Service
    name: mongodb-connect
  port:
    targetPort: 8083-tcp
  wildcardPolicy: None




---



kind: ConfigMap
metadata:
  name: es-connect-config
  labels:    
    app: mirroring
    component: es-connect
    role: config
apiVersion: v1
data:
  es-connect.properties: |-
    name=es-oplog
    connector.class=ESSink
    tasks.max=1
    es.host=nxpassive-elasticsearch
    es.port=9200
    topics=nxmirror-esoplog


  connect.properties: |-
    ##
    # Licensed to the Apache Software Foundation (ASF) under one or more
    # contributor license agreements.  See the NOTICE file distributed with
    # this work for additional information regarding copyright ownership.
    # The ASF licenses this file to You under the Apache License, Version 2.0
    # (the "License"); you may not use this file except in compliance with
    # the License.  You may obtain a copy of the License at
    #
    #    http://www.apache.org/licenses/LICENSE-2.0
    #
    # Unless required by applicable law or agreed to in writing, software
    # distributed under the License is distributed on an "AS IS" BASIS,
    # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
    # See the License for the specific language governing permissions and
    # limitations under the License.
    ##

    # This file contains some of the configurations for the Kafka Connect distributed worker. This file is intended
    # to be used with the examples, and some settings may differ from those used in a production system, especially
    # the `bootstrap.servers` and those specifying replication factors.

    # A list of host/port pairs to use for establishing the initial connection to the Kafka cluster.
    bootstrap.servers=nxpassive-kafka-0.nxpassive-kafka:9092,nxpassive-kafka-1.nxpassive-kafka:9092,nxpassive-kafka-2.nxpassive-kafka:9092

    # unique name for the cluster, used in forming the Connect cluster group. Note that this must not conflict with consumer group IDs
    group.id=connect-es-cluster

    # The converters specify the format of data in Kafka and how to translate it into Connect data. Every Connect user will
    # need to configure these based on the format they want their data in when loaded from or stored into Kafka
    key.converter=org.apache.kafka.connect.storage.StringConverter
    value.converter=org.apache.kafka.connect.converters.ByteArrayConverter
    # Converter-specific settings can be passed in by prefixing the Converter's setting with the converter we want to apply
    # it to
    key.converter.schemas.enable=false
    value.converter.schemas.enable=false

    # The internal converter used for offsets, config, and status data is configurable and must be specified, but most users will
    # always want to use the built-in default. Offset, config, and status data is never visible outside of Kafka Connect in this format.
    internal.key.converter=org.apache.kafka.connect.json.JsonConverter
    internal.value.converter=org.apache.kafka.connect.json.JsonConverter
    internal.key.converter.schemas.enable=false
    internal.value.converter.schemas.enable=false

    # Topic to use for storing offsets. This topic should have many partitions and be replicated and compacted.
    # Kafka Connect will attempt to create the topic automatically when needed, but you can always manually create
    # the topic before starting Kafka Connect if a specific topic configuration is needed.
    # Most users will want to use the built-in default replication factor of 3 or in some cases even specify a larger value.
    # Since this means there must be at least as many brokers as the maximum replication factor used, we'd like to be able
    # to run this example on a single-broker cluster and so here we instead set the replication factor to 1.
    offset.storage.topic=connect-es-offsets
    offset.storage.replication.factor=1
    #offset.storage.partitions=25

    # Topic to use for storing connector and task configurations; note that this should be a single partition, highly replicated,
    # and compacted topic. Kafka Connect will attempt to create the topic automatically when needed, but you can always manually create
    # the topic before starting Kafka Connect if a specific topic configuration is needed.
    # Most users will want to use the built-in default replication factor of 3 or in some cases even specify a larger value.
    # Since this means there must be at least as many brokers as the maximum replication factor used, we'd like to be able
    # to run this example on a single-broker cluster and so here we instead set the replication factor to 1.
    config.storage.topic=connect-ces-onfigs
    config.storage.replication.factor=1

    # Topic to use for storing statuses. This topic can have multiple partitions and should be replicated and compacted.
    # Kafka Connect will attempt to create the topic automatically when needed, but you can always manually create
    # the topic before starting Kafka Connect if a specific topic configuration is needed.
    # Most users will want to use the built-in default replication factor of 3 or in some cases even specify a larger value.
    # Since this means there must be at least as many brokers as the maximum replication factor used, we'd like to be able
    # to run this example on a single-broker cluster and so here we instead set the replication factor to 1.
    status.storage.topic=connect-es-status
    status.storage.replication.factor=1
    #status.storage.partitions=5

    # Flush much faster than normal, which is useful for testing/debugging
    offset.flush.interval.ms=10000

    # These are provided to inform the user about the presence of the REST host and port configs
    # Hostname & Port for the REST API to listen on. If this is set, it will bind to the interface used to listen to requests.
    #rest.host.name=
    #rest.port=8083

    # The Hostname & Port that will be given out to other workers to connect to i.e. URLs that are routable from other servers.
    #rest.advertised.host.name=
    #rest.advertised.port=

    # Set to a list of filesystem paths separated by commas (,) to enable class loading isolation for plugins
    # (connectors, converters, transformations). The list should consist of top level directories that include
    # any combination of:
    # a) directories immediately containing jars with plugins and their dependencies
    # b) uber-jars with plugins and their dependencies
    # c) directories immediately containing the package directory structure of classes of plugins and their dependencies
    # Examples:
    # plugin.path=/usr/local/share/java,/usr/local/share/kafka/plugins,/opt/connectors,
    plugin.path=/plugins
  log4j.properties: |-
    log4j.rootLogger=INFO, stdout
    log4j.appender.stdout=org.apache.log4j.ConsoleAppender
    log4j.appender.stdout.layout=org.apache.log4j.PatternLayout
    log4j.appender.stdout.layout.ConversionPattern=[%d] %p %m (%c)%n

    # Suppress connection log messages, three lines per livenessProbe execution
    log4j.logger.org.apache.zookeeper.server.NIOServerCnxnFactory=WARN
    log4j.logger.org.apache.zookeeper.server.NIOServerCnxn=WARN


---

apiVersion: v1
kind: DeploymentConfig
metadata:
  name: es-connect
  labels:
    app: mirroring    
    component: es-connect
spec:
  replicas: 1
  template:
    metadata:
      labels:
        app: mirroring
        component: es-connect
    spec:      
      containers:
      - name: es-connect
        image: docker-registry.default.svc:5000/nuxeo/mongodb-connect:latest
        env:
          - name: KAFKA_LOG4J_OPTS
            value: -Dlog4j.configuration=file:/etc/kafka/log4j.properties
        ports:
          - containerPort: 8083
        command:
          - ./bin/connect-distributed.sh
          -   /etc/kafka/connect.properties
          -   /etc/kafka/es-connect.properties
        resources:
          requests:
            cpu: 100m
            memory: 512Mi
        volumeMounts:
        - name: config
          mountPath: /etc/kafka          
        - name: logs
          mountPath: /opt/kafka/logs
      volumes:
      - name: config
        configMap:
          name: es-connect-config
      - name: logs
        emptyDir: {}

---

apiVersion: v1
kind: Service
metadata:
  name: es-connect
  labels:
    app: mirroring
spec:
  type: LoadBalancer
  selector:
    app: mirroring
    component: es-connect
  ports:
  - name: 8083-tcp
    port: 8083
    protocol: TCP

---
apiVersion: v1
kind: Route
metadata:
  labels:
    app: mirroring
  name: es-connect-route  
spec:
  # TODO: Add parameter
  host: es-connect.apps.io.nuxeo.com
  to:
    kind: Service
    name: mongodb-connect
  port:
    targetPort: 8083-tcp
  wildcardPolicy: None





